{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Mq3C0ZGp20O"
   },
   "source": [
    "# 『人を知る』人工知能講座 <br> <span style=\"color: #00B0F0;\">Session 3 言語メディア</span> <br> <span style=\"background-color: #1F4E79; color: #FFFFFF;\">&nbsp;3&nbsp;</span> BERTによる自然言語処理 〜日本語Pre-training〜 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KX8dYZnp3yr_"
   },
   "source": [
    "## 1. 日本語Pre-trainedモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jvjp8CvRv9VR"
   },
   "source": [
    "BERT日本語pre-trainedモデルを http://nlp.ist.i.kyoto-u.ac.jp/index.php?BERT%E6%97%A5%E6%9C%AC%E8%AA%9EPretrained%E3%83%A2%E3%83%87%E3%83%AB で公開しています。いくつか公開しているモデルのうち、今回は「通常版: Japanese_L-12_H-768_A-12_E-30_BPE_transformers.zip (393M; 19/11/15公開) 」を使います。zipを解凍し、/data/nlp/tool/Japanese_L-12_H-768_A-12_E-30_BPE_transformers においています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まず、英語pre-trainedモデルと同様に、pre-trainedモデルの中身を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "text",
    "id": "wEdADnVJ4xqN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json  pytorch_model.bin\ttokenizer_config.json  vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls /data/nlp/tool/bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pytorch_model.binがモデルの重みで、config.jsonは設定ファイルです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"attention_probs_dropout_prob\": 0.1,\r\n",
      "  \"hidden_act\": \"gelu\",\r\n",
      "  \"hidden_dropout_prob\": 0.1,\r\n",
      "  \"hidden_size\": 768,\r\n",
      "  \"initializer_range\": 0.02,\r\n",
      "  \"intermediate_size\": 3072,\r\n",
      "  \"max_position_embeddings\": 512,\r\n",
      "  \"num_attention_heads\": 12,\r\n",
      "  \"num_hidden_layers\": 12,\r\n",
      "  \"type_vocab_size\": 2,\r\n",
      "  \"vocab_size\": 32006\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "!cat /data/nlp/tool/bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers/config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vocab.txtが語彙リストです。先頭20行と最後20行を見てみましょう。日本語モデルでは [unused..] を入れていません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD]\r\n",
      "[UNK]\r\n",
      "[CLS]\r\n",
      "[SEP]\r\n",
      "[MASK]\r\n",
      "の\r\n",
      "、\r\n",
      "。\r\n",
      "に\r\n",
      "は\r\n",
      "を\r\n",
      "が\r\n",
      "と\r\n",
      "で\r\n",
      "年\r\n",
      "・\r\n",
      "（\r\n",
      "）\r\n",
      "さ\r\n",
      "して\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 20 /data/nlp/tool/bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers/vocab.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "アドリブ\r\n",
      "変色\r\n",
      "##Ｏｆｆ\r\n",
      "##ドーレ\r\n",
      "いつの\r\n",
      "ＬＳＤ\r\n",
      "アンティオキア\r\n",
      "両性\r\n",
      "##煎\r\n",
      "##彬\r\n",
      "かんする\r\n",
      "##キングス\r\n",
      "フィンガー\r\n",
      "閃\r\n",
      "論点\r\n",
      "インディオ\r\n",
      "スカンジナ\r\n",
      "##紀子\r\n",
      "１６６７\r\n",
      "好調な\r\n"
     ]
    }
   ],
   "source": [
    "!tail -n 20 /data/nlp/tool/bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tokenizer_config.jsonは英語モデルにありませんでしたが、tokenizerのオプションを指定するものです。日本語処理する上では以下が必要です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"do_lower_case\": false, \"tokenize_chinese_chars\": false, \"init_inputs\": []}\r\n"
     ]
    }
   ],
   "source": [
    "#do_lower_case：Trueにしてしまうと、濁点が消えたりとバグる\n",
    "!cat /data/nlp/tool/bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers/tokenizer_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TE9ZVKl1ViYo"
   },
   "source": [
    "本演習ではpre-training自体は行いません。Pre-trainedモデルを使ってmasked language modelを試してみることと、プログラムの中身を見てみます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ETnPHZjQBNbX"
   },
   "source": [
    "## 2. Masked Language Modelの可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "講義の冒頭で説明したmasked language modelは以下のプログラムで試すことができます。詳細はここでは気にしなくて結構です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading .. done.\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "from pyknp import Juman\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Segmentation(object):\n",
    "    \"\"\" Juman++でセグメンテーション \"\"\"\n",
    "    def __init__(self):\n",
    "        self.jumanpp = Juman()\n",
    "        \n",
    "    def segmentation(self, text):\n",
    "        result = self.jumanpp.analysis(text)\n",
    "        text = \" \".join([ mrph.midasi for mrph in result.mrph_list() ])\n",
    "        return text\n",
    "    \n",
    "class MaskedLM(object):\n",
    "    def __init__(self, bert_model_dir, \n",
    "                           topk=5):\n",
    "        self.topk = topk\n",
    "        self.segmentation = Segmentation()\n",
    "  \n",
    "        print(\"loading .. \", end=\"\")\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model_dir, do_lower_case=False)\n",
    "        self.model = BertForMaskedLM.from_pretrained(bert_model_dir)\n",
    "        self.model.to(device)\n",
    "        self.model.eval()\n",
    "        print(\"done.\")\n",
    "    \n",
    "    def get_masked_tokens_list(self, tokens, num_tokens):\n",
    "        masked_tokens_list = []\n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            if i == 0 or i == num_tokens - 1:\n",
    "                continue\n",
    "            new_tokens = copy.deepcopy(tokens)\n",
    "            new_tokens[i] = '[MASK]'\n",
    "\n",
    "            masked_tokens_list.append(new_tokens)\n",
    "            \n",
    "        return masked_tokens_list\n",
    "            \n",
    "    def get_predictions(self, sentence):\n",
    "        print(\"{}\".format(sentence))\n",
    "        text = self.segmentation.segmentation(sentence)\n",
    "        tokens = self.tokenizer.tokenize(text)\n",
    "        \n",
    "        # 先頭に[CLS]、末尾に[SEP]トークンを追加\n",
    "        tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n",
    "        num_tokens = len(tokens)\n",
    "        \n",
    "        # 各単語をMASKする\n",
    "        masked_tokens_list = self.get_masked_tokens_list(tokens, num_tokens)\n",
    "\n",
    "        tokens_tensor = torch.tensor([ self.tokenizer.convert_tokens_to_ids(masked_tokens) for masked_tokens in masked_tokens_list ])\n",
    "        tokens_tensor = tokens_tensor.to(device)\n",
    "        \n",
    "        # ここがメイン\n",
    "        # outputs[0]に各入力トークンにおける各単語の予測確率が入っている\n",
    "        # shape: (batch_size, sequence_length, config.vocab_size)\n",
    "        outputs = self.model(tokens_tensor)\n",
    "        predictions = outputs[0]\n",
    "        \n",
    "        # 予測確率が高い順にソート\n",
    "        _, indices = torch.sort(predictions, descending=True)\n",
    "\n",
    "        prediction_results = []\n",
    "        inputs = []\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            if i == 0 or i == num_tokens - 1:\n",
    "                continue\n",
    "            prediction_string = self.get_prediction_string(indices[i - 1, i, :self.topk].cpu().numpy(), token)\n",
    "            inputs.append(token) \n",
    "            prediction_results.append(prediction_string)\n",
    "\n",
    "        df = pd.DataFrame({'input': inputs, 'prediction': prediction_results})\n",
    "        # indexを1始まりにする\n",
    "        df.index = df.index + 1\n",
    "        return df.style.set_table_styles(\n",
    "                [{'selector': 'th',\n",
    "                  'props': [('text-align', 'center')]}, \n",
    "                 {'selector': 'td',\n",
    "                  'props': [('text-align', 'left')]}]).render()\n",
    "\n",
    "    def get_prediction_string(self, topk_predictions, token):\n",
    "        strings = []\n",
    "        for predicted_token in self.tokenizer.convert_ids_to_tokens(topk_predictions):\n",
    "            # 入力と一致\n",
    "            if token == predicted_token:\n",
    "                strings.append(\"<font color='red'>{}</font>\".format(predicted_token))\n",
    "            else:\n",
    "                strings.append(predicted_token)\n",
    "\n",
    "        return \", \".join(strings)\n",
    "\n",
    "masked_lm = MaskedLM('/data/nlp/tool/bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「loading .. done.」と出たら以下を実行してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習問題 1\n",
    "* 好きな文を入れて、結果を考察してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. プログラムの中身の理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "プログラムの中身を少し理解してみましょう。transformers/transformers/modeling_bert.py に様々なモデルが用意されています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "このうち、BertForSequenceClassificationクラスが1文または文ペア分類タスクが用いられていたものです。(講義スライドの92ページ目の図を参照してください)\n",
    "\n",
    "*   青色で囲った部分がメインです。入力のトークン列をベクトル列に変換します。この関数の中で、講義で説明したself-attention(12層)が動いています。(ここは完全にブラックボックスです)\n",
    "*   緑色の部分で[CLS]に対応するベクトルを取得しています。(=outputs[1])\n",
    "*   赤色の部分は線形の行列を作用し、緑色のベクトル(768次元)からクラス数の次元のベクトルに変換しています。\n",
    "*   その後、クロスエントロピーロスを計算しています。\n",
    "\n",
    "青色の部分をブラックボックスにしているとはいえ、これだけです。タスクによって異なるのは入力のトークン列と最終層(赤色の部分)、ロス関数だけです。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TokyoNLP_BERT.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
