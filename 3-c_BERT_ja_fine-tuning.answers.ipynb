{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Mq3C0ZGp20O"
   },
   "source": [
    "# 『人を知る』人工知能講座 <br> <span style=\"color: #00B0F0;\">Session 3 言語メディア</span> <br> <span style=\"background-color: #1F4E79; color: #FFFFFF;\">&nbsp;3&nbsp;</span> BERTによる自然言語処理 〜日本語Fine-tuning〜 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35j227LZjxoQ"
   },
   "source": [
    "日本語では英語のGLUEデータセットのように大規模な種々のデータセットが整備されているわけではありませんので、タグ付きコーパスからデータセットを生成し、fine-tuningします。\n",
    "\n",
    "本演習では1日目で用いたKNBC (Kyoto University and NTT Blog Corpus)を用います。データは `/data/nlp/text/KNBC_v1.0_090925_utf8` にあります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "j_612HgRG9AE",
    "outputId": "a69ef710-c8ff-4e55-ddda-006e11638415"
   },
   "outputs": [],
   "source": [
    "!ls -l /data/nlp/text/KNBC_v1.0_090925_utf8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J42qVuhql8dk"
   },
   "source": [
    "1日目と同様、アノテーションを可視化できる html を確認してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "colab_type": "code",
    "id": "xiwe3SVwHRuK",
    "outputId": "b7592069-188c-4a85-d959-1481ec7b2c81"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "KNBC_dir = \"/data/nlp/text/KNBC_v1.0_090925_utf8\"\n",
    "HTML(\"<style type='text/css'>\" + open(f\"{KNBC_dir}/html/knbc_article_index.css\").read() + \"</style>\")\n",
    "HTML(open(f\"{KNBC_dir}/html/KN001_Kyoto_1-1-17-01.html\").read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w6w6TR4kp321"
   },
   "source": [
    "このコーパスを使って、以下のタスクをfine-tuningで解きます。\n",
    "\n",
    "* 評判分析\n",
    "* 文書分類\n",
    "* 固有表現解析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RsAjDaKScMWn"
   },
   "source": [
    "## 1. 評判分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QoKEWWRjp-pJ"
   },
   "source": [
    "GLUEの最初で見た SST-2 データセットと全く同じ形式のファイルを生成し、fine-tuningのコマンドを動かします。\n",
    "\n",
    "再掲: KNBCでは「批評」(賛成と反対)や「感情」(好きと嫌い)など、いくつかの軸について、評判を保持している人や評判の対象などが付与されています。ここではfine-tuningを動かすことを目的としますので、かなり荒っぽいですが、「批評」の軸について賛成(「批評＋」と表記されている)を表す表現があれば文全体をpositive、反対(同様に「批評−」)があれば文全体をnegativeとみなすことにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BzIUL8OnZ3bb"
   },
   "source": [
    "### 手順1. 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZ4lgB9qsSMy"
   },
   "source": [
    "上記の基準をもとに以下のpythonスクリプトで評判分析のデータセットを生成します。1日目と同じスクリプトです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "LK73M0YejqdF",
    "outputId": "d5c42549-b070-49b9-8484-17603393ba5f"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def get_words(sid):\n",
    "    # sid: KN001_Keitai_1-1-12-01\n",
    "    # -> KNBC_v1.0_090925/corpus1/KN001_Keitai_1/KN001_Keitai_1-1-12-01\n",
    "    subdirname = (sid.split(\"-\"))[0]\n",
    "    filename = f\"{KNBC_dir}/corpus1/{subdirname}/{sid}\"\n",
    "    if os.path.exists(filename) is False:\n",
    "        return None\n",
    "\n",
    "    # 単語集合を得る (正解単語区切を利用)\n",
    "    words = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as reader:\n",
    "        buf = \"\"\n",
    "        for line in reader.readlines():\n",
    "            if line.startswith((\"#\", \"*\", \"+\", \"EOS\")):\n",
    "                continue\n",
    "            word = (line.split(\" \"))[0]\n",
    "            words.append(word)\n",
    "\n",
    "    return words\n",
    "\n",
    "def get_sentiment_label(sentiment_type):\n",
    "    # 批評＋を含めば1, 批評−を含めば0\n",
    "    # 両方含むものはスキップ\n",
    "    if \"批評＋\" in sentiment_type and \"批評−\" in sentiment_type:\n",
    "        return None\n",
    "    if \"批評＋\" in sentiment_type:\n",
    "        return 1\n",
    "    if \"批評−\" in sentiment_type:\n",
    "        return 0\n",
    "    return None\n",
    "\n",
    "out_dir = \"sentiment_analysis_KNBC\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "f_out = open(f\"{out_dir}/all.tsv\", \"w\")\n",
    "\n",
    "for filename in glob.glob(f\"{KNBC_dir}/corpus2/*\"):\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as reader:\n",
    "        buf = \"\"\n",
    "        for line in reader.readlines():\n",
    "            # 文ID, 文, 評判保持者, 評判表現, 評判タイプ, 評判対象\n",
    "            # KN001_Keitai_1-1-12-01\t確かにプリペイドには、いくつかの弱点がある。\t[著者]\tいくつかの弱点がある\t批評−\tプリペイド\n",
    "            sid, sentence, _, _, sentiment_type, _ = line.strip(\"\\n\").split(\"\\t\")\n",
    "            words = get_words(sid)\n",
    "            if words is None:\n",
    "              # logger.warning(f\"skip: {sid} {sentence}\")\n",
    "              continue\n",
    "            label = get_sentiment_label(sentiment_type)\n",
    "            tokenized_sentence = \" \".join(words)\n",
    "            if label is not None:         \n",
    "                print(f\"{tokenized_sentence}\\t{label}\", file=f_out)\n",
    "\n",
    "f_out.close()\n",
    "\n",
    "print(\"完了!\")\n",
    "!wc -l sentiment_analysis_KNBC/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eRV8x7rOsdJA"
   },
   "source": [
    "680文生成されました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Un-hogUSsa3S"
   },
   "source": [
    "データの中身を見てみます。以下ではランダムに選んだ10件を表形式で表示しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "colab_type": "code",
    "id": "vHrVeMZ3rZpY",
    "outputId": "950d6e7c-bc9a-48ed-8242-944185ac12bd"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data_sentiment = pd.read_csv('sentiment_analysis_KNBC/all.tsv', encoding='utf-8', delimiter='\\t', names=('sentence', 'label'))\n",
    "\n",
    "data_sentiment.sample(10).style.set_table_styles(\n",
    "                [{'selector': 'th',\n",
    "                  'props': [('text-align', 'center')]}, \n",
    "                 {'selector': 'td',\n",
    "                  'props': [('text-align', 'left')]}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDaJvixWtAwM"
   },
   "source": [
    "pandasを使えば以下のようにラベルの頻度を簡単に集計することができます。\"1\"が\"0\"の2倍弱多いことがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "KERJpp3tr-Rq",
    "outputId": "6ff7cfd3-d878-4bca-b1e0-6e65080fce20"
   },
   "outputs": [],
   "source": [
    "data_sentiment[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eMXRFhYQujtW"
   },
   "source": [
    "次にtrain/dev/testに分割 (split)します。ここでは8:1:1の割合で分割します。ここでもpandasを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ilHcLCTLsH2j"
   },
   "outputs": [],
   "source": [
    " import numpy as np\n",
    " \n",
    " def split_train_dev_test(data, dirname, header,\n",
    "                         split_ratios=(0.8, 0.1, 0.1)):\n",
    "\n",
    "    ts = data.shape\n",
    "    df = pd.DataFrame(data)\n",
    "    shuffle_df = df.reindex(np.random.permutation(df.index))\n",
    "\n",
    "    indice_1 = int(ts[0] * split_ratios[0])\n",
    "    indice_2 = int(ts[0] * (split_ratios[0] + split_ratios[1]))\n",
    "\n",
    "    shuffle_df[:indice_1].to_csv(f\"{dirname}/train.tsv\",header=header, sep='\\t', index=False)\n",
    "    shuffle_df[indice_1:indice_2].to_csv(f\"{dirname}/dev.tsv\",header=header, sep='\\t', index=False)\n",
    "    shuffle_df[indice_2:].to_csv(f\"{dirname}/test.tsv\",header=header, sep='\\t', index=False)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mJUwJshJsade"
   },
   "outputs": [],
   "source": [
    "split_train_dev_test(data_sentiment, \"sentiment_analysis_KNBC\", (\"sentence\", \"label\"))\n",
    "print(\"完了!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2CHP0p2Du9Cl"
   },
   "source": [
    "以下のように分割されました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "MmYN6-uuu_2K",
    "outputId": "ba293e8a-1e42-491a-cdd9-f4125eae5dbd"
   },
   "outputs": [],
   "source": [
    "!wc -l sentiment_analysis_KNBC/*.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f2JH2jZavpC_"
   },
   "source": [
    "中身をみてみます。SST-2と同じ形式のファイルができました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "colab_type": "code",
    "id": "S-IjySiMs9pF",
    "outputId": "38bc7cf3-accb-4081-f321-4648e1aeab31"
   },
   "outputs": [],
   "source": [
    "!head sentiment_analysis_KNBC/train.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KX8dYZnp3yr_"
   },
   "source": [
    "### 手順2. Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pJPkYVq830gt"
   },
   "source": [
    "英語のGLUEのfine-tuningと同様に run_glue.py を実行します。変更点は以下のとおりです。\n",
    "*   --model_name_or_pathオプションで日本語pre-trainedモデルを指定 (/data/nlp/tool/bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers)\n",
    "*   --do_lower_caseを削除 (tokenizer_config.jsonでfalseにしていますが念のため)\n",
    "*   --data_dirオプションで先ほど作ったデータのディレクトリを指定\n",
    "\n",
    "SST-2よりもさらにサイズが小さいため、3エポックでも2分程度で終わります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "f6ai0mnO1YJ3",
    "outputId": "1fa0604b-6a1f-42bc-ed4b-b2a4f75b331f"
   },
   "outputs": [],
   "source": [
    "!python ./transformers/examples/run_glue.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path /data/nlp/tool/bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers \\\n",
    "    --task_name \"SST-2\" \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --save_steps 1000 \\\n",
    "    --data_dir sentiment_analysis_KNBC \\\n",
    "    --max_seq_length 128 \\\n",
    "    --per_gpu_eval_batch_size=8   \\\n",
    "    --per_gpu_train_batch_size=8   \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --output_dir KNBC_result/sentiment_analysis/ \\\n",
    "    --overwrite_output_dir \\\n",
    "    --overwrite_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sbUplRep1c8b"
   },
   "source": [
    "一番最後の数字がdevにおける精度 (accuracy)で、80%前後になると思います。1日目のLSTMによるモデルでは精度70%前後でしたので、BERTがいかに強力であるかがおわかりかと思います。\n",
    "\n",
    "一般に、ニューラルネットワークのモデルで高い精度を達成するために大量のトレーニングデータが必要と言われます。しかし、BERTはBi-LSTMのようなこれまでのニューラルネットワークに比べるとそれほどトレーニングデータを必要としません。これは転移学習 (transfer learning)のおかげです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手順3. 予測"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、好きな文を入力し、予測 (prediction)してみましょう。まず、上記で学習したモデルを別のディレクトリにコピーしておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p my_result\n",
    "!cp -pr KNBC_result/sentiment_analysis/ my_result/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記で使用したデータも別のディレクトリにコピーしましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r sentiment_analysis_KNBC my_sentiment_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そして、以下のようにしてdev.tsvを好きな文で上書きしてください。ここではわかち書きは手動で行ってください。ラベルは使いませんが0にしておいてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo -e 'sentence\\tlabel\\nパフェ は 大変 美味しかった 。\\t0' > my_sentiment_analysis/dev.tsv\n",
    "!head my_sentiment_analysis/dev.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のコマンドで予測します。先ほどとの違いは以下です。\n",
    "* --do_train オプションを削除\n",
    "* --save_steps 1000を削除\n",
    "* --data_dir,  --output_dirオプションで今作ったディレクトリを指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./transformers/examples/run_glue.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path /data/nlp/tool/bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers \\\n",
    "    --task_name \"SST-2\" \\\n",
    "    --do_eval \\\n",
    "    --save_steps 1000 \\\n",
    "    --data_dir my_sentiment_analysis \\\n",
    "    --max_seq_length 128 \\\n",
    "    --per_gpu_eval_batch_size=8   \\\n",
    "    --per_gpu_train_batch_size=8   \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --output_dir my_result/sentiment_analysis/ \\\n",
    "    --overwrite_output_dir \\\n",
    "    --overwrite_cache\n",
    "!cat my_result/sentiment_analysis/dev_predictions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "結果が一番下に出ます。いかがでしたでしょうか。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "npgh3ac75cv1"
   },
   "source": [
    "## 2. 文書分類"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EPJKnXs88o5i"
   },
   "source": [
    "次に文書分類タスクを行います。先ほどの評判分析は1か0の2ラベルでしたが、今度は4カテゴリです。また、入力は1文ではなく文章になりますが、単に全部連結したものを入力とし、1文と同様に扱います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abYufupcaCgM"
   },
   "source": [
    "### 手順1. 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P9qWSAPcHyCu"
   },
   "outputs": [],
   "source": [
    "out_dir = \"text_classification_KNBC\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "labels = [ \"Keitai\", \"Kyoto\", \"Gourmet\", \"Sports\"]\n",
    "\n",
    "def get_label(basename):\n",
    "    for label in labels:\n",
    "        if label in basename:\n",
    "            return label\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "f_out = open(f\"{out_dir}/all.tsv\", \"w\")\n",
    "\n",
    "for dir in glob.glob(f\"{KNBC_dir}/corpus1/*\"):\n",
    "    basename = os.path.basename(dir)\n",
    "    words = []\n",
    "    \n",
    "    # for each sentence\n",
    "    sentence_index = 1\n",
    "    while (True):\n",
    "        filename = f\"{dir}/{basename}-1-{sentence_index}-01\"\n",
    "        if os.path.exists(filename) is False:\n",
    "            break\n",
    "\n",
    "        header = True\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as reader:\n",
    "            buf = \"\"\n",
    "            for line in reader.readlines():\n",
    "                if line.startswith((\"#\", \"*\", \"+\", \"EOS\")):\n",
    "                    continue\n",
    "                word = (line.split(\" \"))[0]\n",
    "                \n",
    "                # skip: category (例: [京都観光])\n",
    "                if sentence_index == 1 and header is True:\n",
    "                    continue\n",
    "                if header is True and word == \"]\":\n",
    "                    header = False\n",
    "                words.append(word)\n",
    "        sentence_index += 1\n",
    "\n",
    "    label = get_label(basename)\n",
    "    if label is not None:\n",
    "        print(\"{}\\t{}\".format(\" \".join(words), label), file=f_out)\n",
    "    \n",
    "f_out.close()\n",
    "print(\"完了!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "しばらくは評判分析と同様のコマンドですので、どんどんコマンドを実行しながら内容を確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "colab_type": "code",
    "id": "s2-aHDy1eVV0",
    "outputId": "bc8dde8a-8b0b-46a0-aa9e-d2cf3dc72f12"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data_text_classification = pd.read_csv('text_classification_KNBC/all.tsv', encoding='utf-8', delimiter='\\t', names=('sentence', 'label'))\n",
    "data_text_classification.sample(10).style.set_table_styles(\n",
    "                [{'selector': 'th',\n",
    "                  'props': [('text-align', 'center')]}, \n",
    "                 {'selector': 'td',\n",
    "                  'props': [('text-align', 'left')]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "68MzXk_C8YKF",
    "outputId": "e086ec57-9573-4e22-de8a-db8c08c9d9e8"
   },
   "outputs": [],
   "source": [
    "data_text_classification[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U-dZV0egClOc"
   },
   "outputs": [],
   "source": [
    "split_train_dev_test(data_text_classification, \"text_classification_KNBC\", (\"sentence\", \"label\"))\n",
    "print(\"完了!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "colab_type": "code",
    "id": "NTIG5DdKET6n",
    "outputId": "4d0f860b-7786-48f2-85da-b56a87409820"
   },
   "outputs": [],
   "source": [
    "!wc -l text_classification_KNBC/*.tsv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uUVS--6S9Urn"
   },
   "source": [
    "ここまでは評判分析と全く同じです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uUVS--6S9Urn"
   },
   "source": [
    "### 練習問題 1\n",
    "ラベルが変わったことによりデータを読みこむ部分を追加しなければいけません。データの読みこみは transformers/transformers/data/processors/glue.py で行っています。このファイルを編集するにはファイル一覧の画面に移動して、ファイル名をクリックするとファイルを開くことができます。\n",
    "\n",
    "SST-2は以下のクラス Sst2Processor でデータの読みこみをしています。これを真似して TextClassificationKNBCProcessor を作ってみましょう。317行目あたりにSst2Processorをコピーして TextClassificationKNBCProcessor を作っていますので、関数 get_labels のところだけを変更してください。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "HvULnceNC7bh",
    "outputId": "f7258068-8f3a-44b8-f824-120ccc3e1083"
   },
   "source": [
    "class Sst2Processor(DataProcessor):\n",
    "    \"\"\"Processor for the SST-2 data set (GLUE version).\"\"\"\n",
    "\n",
    "    def get_example_from_tensor_dict(self, tensor_dict):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return InputExample(tensor_dict['idx'].numpy(),\n",
    "                            tensor_dict['sentence'].numpy().decode('utf-8'),\n",
    "                            None,\n",
    "                            str(tensor_dict['label'].numpy()))\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            text_a = line[0]\n",
    "            label = line[1]\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JAZYzU6nECLX"
   },
   "source": [
    "あとは同じファイルの末尾に、ラベルの数、どのプロセッサを使うのか、タスクが分類(classification)か回帰(regression)であるかを指定するところがあります。今回は時間の都合上、すでに以下のように追加しています。タスク名は tc-knbc としています(tcはtext classificationの意味)。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-BR_Up0EE63"
   },
   "source": [
    "glue_tasks_num_labels = {\n",
    "    \"cola\": 2,\n",
    "    \"mnli\": 3,\n",
    "    \"mrpc\": 2,\n",
    "    \"sst-2\": 2,\n",
    "    \"sts-b\": 1,\n",
    "    \"qqp\": 2,\n",
    "    \"qnli\": 2,\n",
    "    \"rte\": 2,\n",
    "    \"wnli\": 2,\n",
    "    \"tc-knbc\": 4 ## これを追加\n",
    "}\n",
    "\n",
    "glue_processors = {\n",
    "    \"cola\": ColaProcessor,\n",
    "    \"mnli\": MnliProcessor,\n",
    "    \"mnli-mm\": MnliMismatchedProcessor,\n",
    "    \"mrpc\": MrpcProcessor,\n",
    "    \"sst-2\": Sst2Processor, \n",
    "    \"sts-b\": StsbProcessor,\n",
    "    \"qqp\": QqpProcessor,\n",
    "    \"qnli\": QnliProcessor,\n",
    "    \"rte\": RteProcessor,\n",
    "    \"wnli\": WnliProcessor,\n",
    "    \"tc-knbc\": TextClassificationKNBCProcessor, ## これを追加\n",
    "}\n",
    "\n",
    "glue_output_modes = {\n",
    "    \"cola\": \"classification\",\n",
    "    \"mnli\": \"classification\",\n",
    "    \"mnli-mm\": \"classification\",\n",
    "    \"mrpc\": \"classification\",\n",
    "    \"sst-2\": \"classification\",\n",
    "    \"sts-b\": \"regression\",\n",
    "    \"qqp\": \"classification\",\n",
    "    \"qnli\": \"classification\",\n",
    "    \"rte\": \"classification\",\n",
    "    \"wnli\": \"classification\",\n",
    "    \"tc-knbc\": \"classification\", ## これを追加\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ファイルをsaveしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sysnRNMBGwSn"
   },
   "source": [
    "あと、transformers/transformers/data/metrics/\\_\\_init\\_\\_.py で何を評価尺度として用いるかを指定しているところがあります。これも以下のようにすでに追加しています。"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9B7arkZ0G-gI"
   },
   "source": [
    "    def glue_compute_metrics(task_name, preds, labels):\n",
    "        assert len(preds) == len(labels)\n",
    "        if task_name == \"cola\":\n",
    "            return {\"mcc\": matthews_corrcoef(labels, preds)}\n",
    "        elif task_name == \"sst-2\":\n",
    "            return {\"acc\": simple_accuracy(preds, labels)}\n",
    "        elif task_name == \"mrpc\":\n",
    "            return acc_and_f1(preds, labels)\n",
    "        elif task_name == \"sts-b\":\n",
    "            return pearson_and_spearman(preds, labels)\n",
    "        elif task_name == \"qqp\":\n",
    "            return acc_and_f1(preds, labels)\n",
    "        elif task_name == \"mnli\":\n",
    "            return {\"acc\": simple_accuracy(preds, labels)}\n",
    "        elif task_name == \"mnli-mm\":\n",
    "            return {\"acc\": simple_accuracy(preds, labels)}\n",
    "        elif task_name == \"qnli\":\n",
    "            return {\"acc\": simple_accuracy(preds, labels)}\n",
    "        elif task_name == \"rte\":\n",
    "            return {\"acc\": simple_accuracy(preds, labels)}\n",
    "        elif task_name == \"wnli\":\n",
    "            return {\"acc\": simple_accuracy(preds, labels)}\n",
    "        ### ここを追加\n",
    "        elif task_name == \"tc-knbc\":\n",
    "            return {\"acc\": simple_accuracy(preds, labels)}            \n",
    "        else:\n",
    "            raise KeyError(task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jlfOWUwpHbRC"
   },
   "source": [
    "transformers/transformers以下のファイルを更新した場合はpipで再インストールする必要がありますので以下を実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "colab_type": "code",
    "id": "vF3_ZDXsFgba",
    "outputId": "18963e18-a4c1-4b02-9dbe-9e7b60b687d8"
   },
   "outputs": [],
   "source": [
    "!pip install transformers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 手順2. Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YFAQDdECIUNS"
   },
   "source": [
    "準備が整ったのでfine-tuningしましょう。--task_nameで\"tc-knbc\"を指定し、--output_dirで KNBC_result/text_classification/ を指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "S7nXdnQRE6ht",
    "outputId": "6794dc5e-11c8-4a19-8d41-4faf2187722f"
   },
   "outputs": [],
   "source": [
    "!python ./transformers/examples/run_glue.py \\\n",
    "    --model_type bert \\\n",
    "    --model_name_or_path /data/nlp/tool/bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers \\\n",
    "    --task_name \"tc-knbc\" \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --save_steps 1000 \\\n",
    "    --data_dir text_classification_KNBC \\\n",
    "    --max_seq_length 128 \\\n",
    "    --per_gpu_eval_batch_size=8   \\\n",
    "    --per_gpu_train_batch_size=8   \\\n",
    "    --learning_rate 2e-5 \\\n",
    "    --num_train_epochs 3.0 \\\n",
    "    --output_dir KNBC_result/text_classification/ \\\n",
    "    --overwrite_output_dir \\\n",
    "    --overwrite_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rf1JihYKIG_f"
   },
   "source": [
    "3エポック回して約1分で終わります。4カテゴリが結構はっきり異なるジャンルなので90%前後と、高い精度で分類できました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dx4gzsotAbCP"
   },
   "source": [
    "## 3. 固有表現抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k3Ln8Tl69WPC"
   },
   "source": [
    "最後に固有表現抽出 (Named Entity Recognition, NER) を行います。固有表現抽出とはテキスト中の人名、地名、組織名などの固有表現 (Named Entity)を抽出するタスクです。\n",
    "\n",
    "これまでの講義でも説明されたとおり、固有表現抽出は系列ラベリングと呼ばれる手法で解かれることが多いです。以下の例で説明します。以下の文では「太郎」がPERSON、「京都大学」がORGANIZATIONです。「太郎」は1形態素ですが、「京都大学」は2形態素からなります。形態素が固有表現の場合、ラベルの頭にB (Begin)またはI (Inside)を付与し、固有表現でない場合、O (Outside)とし、各形態素のラベルを推定する問題となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W0ngAQ9RBjwD"
   },
   "source": [
    "単語 | 固有表現ラベル\n",
    "--- | ---\n",
    "太郎 | B-PERSON\n",
    "は | O\n",
    "京都 | B-ORGANIZATION\n",
    "大学 | I-ORGANIZATION\n",
    "に | O\n",
    "行った | O"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8AJMdSG8ESQd"
   },
   "source": [
    "日本語の固有表現解析ではIREX (Information Retrieve and Extraction Exercise)で定義された、組織名(ORGANIZATION), 人名(PERSON), 地名(LOCATION), 固有物名(ARTIFACT), 日付表現(DATE), 時間表現(TIME), 金額表現(MONEY), 割合表現(PERCENT)の8種類を対象とすることが多いです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cSoO6RvNaG7Z"
   },
   "source": [
    "### 手順1. 前処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4arTwUSx8-9Z"
   },
   "source": [
    "以下のpythonスクリプトで固有表現データセットを生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-UJAXs-qINzJ"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import logging\n",
    "import re\n",
    "import random\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "corpus_types = [ \"train\", \"dev\", \"test\" ]\n",
    "os.makedirs(\"ner_KNBC\", exist_ok=True)\n",
    "fs_out = { corpus_type: open(f\"ner_KNBC/{corpus_type}.txt\", \"w\") for corpus_type in corpus_types }\n",
    "\n",
    "ne_pat = re.compile(r\"<NE:(.+?):(.+?)>\")\n",
    "\n",
    "def get_corpus_type():\n",
    "    # train:dev:test = 8:1:1 に split する\n",
    "    rand = random.random()\n",
    "    \n",
    "    if rand >= 0.2:\n",
    "        return \"train\"\n",
    "    elif 0.1 <= rand < 0.2:\n",
    "        return \"dev\"\n",
    "    else:\n",
    "        return \"test\"\n",
    "\n",
    "corpus1_dirname = f\"{KNBC_dir}/corpus1\"\n",
    "\n",
    "doc_num = 0\n",
    "for dir in glob.glob(f\"{corpus1_dirname}/*\"):\n",
    "    basename = os.path.basename(dir)\n",
    "    corpus_type = get_corpus_type()\n",
    "\n",
    "    f_out = fs_out[corpus_type]\n",
    "\n",
    "    # for each sentence\n",
    "    sentence_index = 1\n",
    "    while (True):\n",
    "        filename = f\"{dir}/{basename}-1-{sentence_index}-01\"\n",
    "        if os.path.exists(filename) is False:\n",
    "            break\n",
    "\n",
    "        words, ne_labels = [], []\n",
    "        with open(filename, \"r\", encoding=\"utf-8\") as reader:\n",
    "            buf = \"\"\n",
    "            for line in reader.readlines():\n",
    "                if line.startswith((\"#\", \"*\", \"+\", \"EOS\")):\n",
    "                    continue\n",
    "                word = (line.split(\" \"))[0]\n",
    "                words.append(word)\n",
    "\n",
    "                # 例：黒田 くろだ 黒田 名詞 6 人名 5 * 0 * 0 \"疑似代表表記 代表表記:黒田/くろた\" <疑似代表表記>..<NE:PERSON:head>\n",
    "                m = ne_pat.search(line)\n",
    "                if m:\n",
    "                    category, position = m.groups()\n",
    "                    if category == \"OPTIONAL\":\n",
    "                        label = \"O\"\n",
    "                    else:\n",
    "                        if position == \"head\" or position == \"single\":\n",
    "                            position_label = \"B\"\n",
    "                        else:\n",
    "                            position_label = \"I\"\n",
    "                        label = f\"{position_label}-{category}\"\n",
    "                else:\n",
    "                    label = \"O\"\n",
    "                ne_labels.append(label)\n",
    "\n",
    "            for word, ne_label in zip(words, ne_labels):\n",
    "                print(f\"{word} {ne_label}\", file=f_out)\n",
    "            print(file=f_out)\n",
    "\n",
    "        sentence_index += 1\n",
    "\n",
    "for corpus_type in corpus_types:\n",
    "    fs_out[corpus_type].close()\n",
    "\n",
    "print(\"完了!\")\n",
    "!wc -l ner_KNBC/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9SChLB239PlC"
   },
   "source": [
    "中身を見てみます。1カラム目が見出し、2カラム目が固有表現ラベルになっていて、空行が文区切りを表わします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 903
    },
    "colab_type": "code",
    "id": "F8l9qbcdQUnx",
    "outputId": "041886b7-63ea-4c30-82e7-0084fa601201"
   },
   "outputs": [],
   "source": [
    "!head -n 50 ner_KNBC/dev.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5VrWnF0RBTAV"
   },
   "source": [
    "### 練習問題 2\n",
    "fine-tuningの前に、ラベル一覧を得ておく必要があります。1行に1ラベルとし、ner_KNBC/labels.txtに保存してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "colab_type": "code",
    "id": "xbvkhdtRQ6Xi",
    "outputId": "e9b45dad-ba90-4160-ef6f-7e5748e04381"
   },
   "outputs": [],
   "source": [
    "########## ここにコマンドを書いて下さい\n",
    "!cat ner_KNBC/train.txt | grep -v \"^$\" | cut -d \" \" -f 2 | sort | uniq > ner_KNBC/labels.txt\n",
    "!cat ner_KNBC/labels.txt\n",
    "##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SBygnltsaLWK"
   },
   "source": [
    "### 手順2. Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xgArMHwhPNOe"
   },
   "source": [
    "系列ラベリングのfine-tuningは run_ner.py というスクリプトを使います。オプションはこれまでとほぼ同様です。--labelsオプションで上で生成したlabel一覧のファイルを指定するくらいが異なることです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "L8U6WROqRoIB",
    "outputId": "3cc8925f-3226-400a-9bf5-10481af72f6b"
   },
   "outputs": [],
   "source": [
    "!python ./transformers/examples/run_ner.py --data_dir ./ner_KNBC/ \\\n",
    "--model_type bert \\\n",
    "--labels ./ner_KNBC/labels.txt \\\n",
    "--model_name_or_path /data/nlp/tool/bert/Japanese_L-12_H-768_A-12_E-30_BPE_transformers \\\n",
    "--output_dir KNBC_result/ner \\\n",
    "--max_seq_length 128 \\\n",
    "--num_train_epochs 3 \\\n",
    "--per_gpu_train_batch_size 16 \\\n",
    "--save_steps 1000 \\\n",
    "--seed 1 \\\n",
    "--do_train \\\n",
    "--do_eval \\\n",
    "--do_predict \\\n",
    "--overwrite_output_dir \\\n",
    "--overwrite_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3エポックで10分弱かかります。この間に、GPU使用状況を確認できる nvidia-smi コマンドを使ってみましょう。\n",
    "2日目にJuman++を使った時と同じように、New → Terminal で Terminal を開き、 `nvidia-smi` と打ってみてください。\n",
    "重要なのは以下です。\n",
    "* Volatile GPU-Util: GPUがどれくらい使われているか。100%に近いほどよい\n",
    "* GPU Memory Usage: GPUメモリがどれくらい使われているか。他に制約がなければ最大に近いまで使うとよい\n",
    "\n",
    "しばらく実行し続けるときは例えば `nvidia-smi -l 3` と打つと3秒ごとに実行されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bdm95awfvbxA"
   },
   "source": [
    "最後に出ている数字が test での精度です。F値で 0.75 程度です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bdm95awfvbxA"
   },
   "source": [
    "システムの出力を簡単に確認してみましょう。KNBC_result/ner/test_predictions.txt がシステムの出力です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -20 KNBC_result/ner/test_predictions.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 練習問題 3*\n",
    "上記の run_ner.py のオプション --per_gpu_train_batch_size はトレーニング時のバッチサイズを指定するものです。これは128や4にして実行してみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uKF162GNCqca"
   },
   "source": [
    "### 手順3. 結果の可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z2jlWEKaCwLB"
   },
   "source": [
    "システムの出力を検討する上で結果をわかりやすく表示することは非常に重要です。ここでは spacy というライブラリが提供している displacy を使って、固有表現解析の結果を可視化してみます。spacy はpipで簡単にインストールすることができます。(以下で使うtermcolorというライラブリも一緒にインストールしておきます)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy termcolor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2rUK8Q_wDJxO"
   },
   "source": [
    "displacy では以下のように文 (text)と固有表現の集合 (ents)を与えることによって、可視化することができます。「ents」のそれぞれの固有表現のstart/endはそれぞれ文頭からの文字数を表しています。(endは固有表現の末尾の文字位置 + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "18FsaW9ua9vQ",
    "outputId": "16dc6484-c53f-494a-dc11-86ca65f3286c"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "ex = [{\"text\": \"太郎 は 京都 大学 に 通っている 。\",\n",
    "       \"ents\": [{\"start\": 0, \"end\": 2, \"label\": \"PERSON\"},\n",
    "                {\"start\": 5, \"end\": 10, \"label\": \"ORG\"}]}\n",
    "      ]\n",
    "displacy.render(ex, style=\"ent\", manual=True, jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSIM2QAbSadv"
   },
   "source": [
    "KNBCはコーパスサイズが小さかったためF値が0.75程度でしたが、固有表現解析で標準的に用いられているCRL固有表現データ(約1万文)ではF値0.92程度になります。古典的機械学習手法を用いた[笹野ら08]ではF値0.89と報告されていますので、固有表現解析でもBERTが強力であることがわかります。あらかじめ学習を走らせておき、その結果を /data/nlp/tool/bert/CRL 以下においていますので、これを用いて結果を可視化してみましょう。\n",
    "**(これは毎日新聞のデータですので、持ち出さないようにお願いします)**\n",
    "\n",
    "以下のpythonコードでシステムの出力と正解を可視化します。文単位でシステムの出力と正解が一致する場合はシステムの出力(=正解)を表示し、1文のどこかが異なる場合は上にシステムの出力、下に正解を表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "ONCICL0zb45b",
    "outputId": "1c9211c1-bc2c-4d06-e1a9-de5ac11bc1e6"
   },
   "outputs": [],
   "source": [
    "from seqeval.metrics.sequence_labeling import get_entities\n",
    "import termcolor\n",
    "\n",
    "tag_conversion_map = { \"ORGANIZATION\": \"ORG\",\n",
    "                       \"LOCATION\": \"LOC\"}\n",
    "class Word(object):\n",
    "    def __init__(self, string, ner_tag, offset):\n",
    "        self.string = string\n",
    "        self.ner_tag = ner_tag\n",
    "        self.start = offset\n",
    "        self.end = offset + len(string)\n",
    "\n",
    "def get_ner_example(lines):\n",
    "    words = []\n",
    "    offset = 0\n",
    "    for line in lines:\n",
    "        string, ner_tag = line.split(\" \")\n",
    "        word = Word(string, ner_tag, offset)\n",
    "        words.append(word)\n",
    "        # +1 は空白の分\n",
    "        offset += len(word.string) + 1\n",
    "\n",
    "    # [('PER', 0, 1), ('LOC', 3, 3)]\n",
    "    entities = get_entities([ word.ner_tag for word in words ])\n",
    "    spacy_entities = []\n",
    "    for entity in entities:\n",
    "        ner_label, start_word_index, end_word_index = entity \n",
    "        spacy_entities.append({ \"start\": words[start_word_index].start,\n",
    "                                                    \"end\": words[end_word_index].end, \n",
    "                                                    \"label\": tag_conversion_map[ner_label] if ner_label in tag_conversion_map else ner_label })  \n",
    "        \n",
    "    return { \"text\": \" \".join([ word.string for word in words ]),\n",
    "                  \"ents\": spacy_entities }  \n",
    "\n",
    "def get_ner_examples(filename):\n",
    "    examples = []\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as reader:\n",
    "        lines = [] \n",
    "        for line in reader.readlines():\n",
    "            line = line.rstrip(\"\\n\")\n",
    "      \n",
    "            if line:\n",
    "                lines.append(line)\n",
    "            # 空行 (文の切れ目)\n",
    "            else:\n",
    "                example = get_ner_example(lines)\n",
    "                examples.append(example)\n",
    "                lines = []\n",
    "\n",
    "    return examples\n",
    "\n",
    "def is_same_example(system_example, gold_example):\n",
    "    assert system_example[\"text\"] == gold_example[\"text\"]\n",
    "\n",
    "    # entityの数が異なる\n",
    "    if len(system_example[\"ents\"]) != len(gold_example[\"ents\"]):\n",
    "        return False\n",
    "    \n",
    "    for system_ent, gold_ent in zip(system_example[\"ents\"], gold_example[\"ents\"]):\n",
    "        # start, end, labelのいずれかが異なる\n",
    "        if system_ent[\"start\"] != gold_ent[\"start\"] or system_ent[\"end\"] != gold_ent[\"end\"] or system_ent[\"label\"] != gold_ent[\"label\"]:\n",
    "            return False\n",
    "\n",
    "    # すべて一致したので全体が一致\n",
    "        return True\n",
    "\n",
    "def display_result(system_filename, gold_filename):\n",
    "    system_examples = get_ner_examples(system_filename)\n",
    "    gold_examples = get_ner_examples(gold_filename)\n",
    "\n",
    "    for i, (system_example, gold_example) in enumerate(zip(system_examples, gold_examples)):\n",
    "        if i == 100:\n",
    "            break\n",
    "        if is_same_example(system_example, gold_example) is True:\n",
    "            displacy.render(system_example, style=\"ent\", manual=True, jupyter=True)\n",
    "        else:\n",
    "            print(\"-\" * 100)\n",
    "            print(termcolor.colored(\"system\", \"blue\"))\n",
    "            displacy.render(system_example, style=\"ent\", manual=True, jupyter=True)\n",
    "            print(termcolor.colored(\"gold\", \"red\"))\n",
    "            displacy.render(gold_example, style=\"ent\", manual=True, jupyter=True)\n",
    "            print(\"-\" * 100)\n",
    "      \n",
    "display_result(\"/data/nlp/tool/bert/CRL/test_predictions.txt\", \"/data/nlp/tool/bert/CRL/test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jCpskc3EEe3m"
   },
   "source": [
    "### 練習問題 4\n",
    "上記の結果からどのようなことがわかるか分析してみよう。\n",
    "\n",
    "*   システムはこんな難しいものでもわかるのか\n",
    "*   逆にこんなやさしいものもわからないのか\n",
    "*   システムがわからなくても仕方ない難しいもの\n",
    "*   システムの出力の方が正しくて正解が間違っているのではないかというもの\n",
    "*   タグ付けが誤っているのではないか\n",
    "\n",
    "\n",
    "以上のように、BERTによるfine-tuningではプログラムの中身をほぼ変更することなく、入力データを用意するだけでなく、様々なタスクのfine-tuningを行うことができます。"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "TokyoNLP_BERT.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
